#!/bin/bash

{# Parameters for the job scheduler SLURM, see https://support.ceci-hpc.be/doc/_contents/QuickStart/SubmittingJobs/SlurmTutorial.html #}
#SBATCH --output=slurm_output.log
#SBATCH --job-name={{  mol_name  }}_orca
#SBATCH --mail-user={{  user_email  }}
#SBATCH --mail-type={{  mail_type  }}
#SBATCH --time={{  job_duration  }}
#SBATCH --ntasks={{  job_cores  }}
{% if partition != "default" %}
#SBATCH --partition={{  partition  }}
{% endif %}

echo -e "Renaming the original .xyz file to avoid overwriting it.\n"
cd $SLURM_SUBMIT_DIR
mv {{  mol_name  }}.xyz {{  mol_name  }}_ori.xyz

{# Job instructions for loading and running ORCA, and defining the ORCA input and output files. 
A temporary directory (SCRATCH) is created on the node where the job is running, for handling temporary files. 
See https://support.ceci-hpc.be/doc/_contents/SubmittingJobs/SlurmFAQ.html#q11-how-do-i-use-the-local-scratch-space for more details. #}

SCRATCH=$LOCALSCRATCH/$SLURM_JOB_ID

echo -e "Creating temporary folder $SCRATCH for handling temporary files.\n"
mkdir -p $SCRATCH || exit $?
cp $SLURM_SUBMIT_DIR/{{  mol_name  }}.inp $SCRATCH/ || exit $?

cd $SCRATCH

echo -e "============================================================================="
echo -e "========================= ORCA execution begins now ========================="
echo -e "=============================================================================\n"

{% for set_env_line in set_env -%}
{{  set_env_line  }}
{% endfor -%}
{{  command  }} {{  mol_name  }}.inp > $SLURM_SUBMIT_DIR/{{  mol_name  }}.out  || exit $?

echo -e "============================================================================="
echo -e "=========================  ORCA execution ends now  ========================="
echo -e "=============================================================================\n"

echo -e "Copying ORCA output files to the submit folder.\n"
cp -r $SCRATCH/* $SLURM_SUBMIT_DIR/ || exit $?

echo -e "Removing $SCRATCH folder.\n"
rm -rf $SCRATCH || exit $?

{# Quality control (was there any problem with ORCA?) #}
cd $SLURM_SUBMIT_DIR
source {{  codes_folder  }}/load_modules.sh
python {{  codes_folder  }}/{{  check_script  }} {{  mol_name  }}.out  || exit $?

{# Job instructions for sending the optimized geometry to the orca output folder for use in the next step. #}
echo -e "\nCopying optimized geometry to {{  output_folder  }}.\n"
mkdir -p {{  output_folder  }}
cp {{  mol_name  }}.xyz {{  output_folder  }}/

{# Job instructions for copying and renaming the configuration file to the results folder. #}
echo -e "Copying and renaming main configuration file to {{  results_folder  }}/{{  mol_name  }}."
mkdir -p {{  results_folder  }}/{{  mol_name  }}
cp {{ config_file  }} {{  results_folder  }}/{{  mol_name  }}/
mv {{  results_folder  }}/{{  mol_name  }}/{{ config_file  }} {{  results_folder  }}/{{  mol_name  }}/{{  mol_name  }}_config.yml

echo -e "   => Hydra specific: copying the main configuration file to {{  output_folder  }} so that the crontab can pick it up with the output.\n"
cp {{  results_folder  }}/{{  mol_name  }}/{{  mol_name  }}_config.yml {{  output_folder  }}/

{# Job instructions for archiving the results into the results folder. #}
echo -e "Archiving output files to {{  results_folder  }}/{{  mol_name  }}/ORCA.\n"
mkdir -p {{  results_folder  }}/{{  mol_name  }}/ORCA
cp {{  mol_name  }}.inp {{  mol_name  }}.out {{  mol_name  }}.xyz {{  mol_name  }}_ori.xyz {{  job_manifest  }} slurm_output.log {{  results_folder  }}/{{  mol_name  }}/ORCA/

echo -e "End of the job."