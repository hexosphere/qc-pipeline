# CECI Clusters relevant informations, see http://www.ceci-hpc.be/clusters.html for more details 
# This is a YAML database, see https://yaml.org/ for more informations

#! Keep in mind the definition of your scaling function when assigning values to the scale_limit keys

# Those keys are shared between the clusters, they will be used for information for which the cluster doesn't matter
shared: 
  progs:
    orca:
      manifest_template: &orca_jinja orca_job.sh.jinja # Name of the jinja template for the job submitting script
      manifest_render: &orca_render orca_job.sh        # Name of the rendered job submitting script
      job_scales: &tiny_orca
        label: tiny
        scale_limit: 50
        partition_name: default
        time: 0-00:10:00
        cores: 8 
        mem_per_cpu: 200 # in MB
        delay_command:
    qchem: 
      job_scales: &tiny_qchem
        label: tiny
        scale_limit: 50
        partition_name: default
        time: 0-00:10:00
        cores: 8 
        mem_per_cpu: 200 # in MB
        delay_command:
    qoct-ra: 
      job_scales: &unique_qoctra
        label: unique
        scale_limit: 100
        partition_name: default
        time: 1-00:00:00
        mem_per_cpu: 2000 # in MB

# Below are all the useful informations specific to each cluster and each program you wish to run on this cluster.

dragon1:
  name: dragon1
  host: UMons
  address: dragon1.umons.ac.be
  subcommand: sbatch
  progs:
    orca:
      manifest_template: *orca_jinja
      manifest_render: *orca_render
      set_env:
        - module --force purge
        - module load orca/4.0.1.2 
      command: /usr/local/orca/orca_4_0_1_2_linux_x86-64_openmpi202/orca
      job_scales: 
        - <<: *tiny_orca     
        - label: small
          scale_limit: 1000
          partition_name: Def
          time: 5-00:00:00
          cores: 8
          mem_per_cpu: 1000 # in MB
          delay_command:
        - label: medium
          scale_limit: 1500
          partition_name: Def
          time: 5-00:00:00
          cores: 8
          mem_per_cpu: 2000 # in MB
          delay_command: --begin=now+60
        - label: big
          scale_limit: 2000
          partition_name: Long
          time: 15-00:00:00
          cores: 8
          mem_per_cpu: 4000 # in MB  
          delay_command: --begin=now+120
    qoct-ra:
      set_env:
      command:
      job_scales:
        - <<: *unique_qoctra

dragon2:
  name: dragon2
  host: UMons
  address: dragon2.umons.ac.be  
  subcommand: sbatch
  progs:
    orca:
      manifest_template: *orca_jinja
      manifest_render: *orca_render
      set_env:
        - module --force purge
        - module load releases/2019b
        - module load ORCA/4.2.1-gompi-2019b 
      command: /opt/cecisw/arch/easybuild/2019b/software/ORCA/4.2.1-gompi-2019b/orca
      job_scales:
        - <<: *tiny_orca
          partition_name: debug  
        - label: small
          scale_limit: 1000
          partition_name: batch
          time: 2-00:00:00
          cores: 8
          mem_per_cpu: 1000 # in MB
          delay_command: 
        - label: medium
          scale_limit: 1500
          partition_name: batch
          time: 5-00:00:00
          cores: 8
          mem_per_cpu: 2000 # in MB
          delay_command: --begin=now+60
        - label: big
          scale_limit: 2000
          partition_name: long
          time: 15-00:00:00
          cores: 8  
          mem_per_cpu: 4000 # in MB
          delay_command: --begin=now+120
    qoct-ra:
      set_env:
      command:
      job_scales:
        - <<: *unique_qoctra

lemaitre3:
  name: lemaitre3
  host: UCL
  address: lemaitre3.cism.ucl.ac.be
  subcommand: sbatch
  progs:
    orca:
      manifest_template: *orca_jinja
      manifest_render: *orca_render
      set_env: 
        - module load ORCA/4.1.0-OpenMPI-3.1.3
      command: /opt/cecisw/arch/easybuild/2018b/software/ORCA/4.1.0-OpenMPI-3.1.3/orca
      job_scales:
        - <<: *tiny_orca  
        - label: small
          scale_limit: 1000
          partition_name: batch
          time: 2-00:00:00
          cores: 8
          mem_per_cpu: 1000 # in MB
          delay_command:
        - label: medium
          scale_limit: 1500
          partition_name: batch
          time: 2-00:00:00
          cores: 8
          mem_per_cpu: 2000 # in MB
          delay_command: --begin=now+60
        - label: big
          scale_limit: 2000
          partition_name: batch
          time: 2-00:00:00
          cores: 8
          mem_per_cpu: 4000 # in MB
          delay_command: --begin=now+120

vega:
  name: vega
  host: ULB
  address: vega.ulb.ac.be
  subcommand: sbatch
  progs:
    orca:
      manifest_template: *orca_jinja
      manifest_render: *orca_render
      set_env: 
        - module purge
        - module load ORCA/4.0.0.2-OpenMPI-2.0.2
      command: /apps/brussel/interlagos/software/ORCA/4.0.0.2-OpenMPI-2.0.2/orca
      job_scales:
        - <<: *tiny_orca  
        - label: small
          scale_limit: 1e3
          partition_name: defq
          time: 5-00:00:00
          cores: 8
          mem_per_cpu: 1000 # in MB
          delay_command: --begin=now+60
        - label: medium
          scale_limit: 1.5e3
          partition_name: defq
          time: 10-00:00:00
          cores: 8
          mem_per_cpu: 2000 # in MB
          delay_command: --begin=now+120
    qoct-ra:
      set_env:
      command:
      job_scales:
        - <<: *unique_qoctra

#Careful, not member of the CECI and uses Torque instead of SLURM
hydra:
  name: hydra
  host: ULB
  address: hydra.ulb.ac.be
  mail-type: a
  subcommand: qsub
  progs:
    qchem:
      manifest_template: qchem_job.sh.jinja
      manifest_render: qchem_job.sh
      set_env: 
        - module load Q-Chem/5.2.1-intel-2019a-mpich3
      command: qchem -nt
      job_scales:
        - <<: *tiny_qchem  
        - label: small
          scale_limit: 500
          partition_name: dummy
          time: 120:00:00
          cores: 8
        - label: medium
          scale_limit: 1000
          partition_name: dummy
          time: 120:00:00
          cores: 16
  