# CECI Clusters relevant informations, see http://www.ceci-hpc.be/clusters.html for more details 
# This is a YAML database, see https://yaml.org/ for more informations

dragon1:
  name: dragon1
  host: UMons
  address: dragon1.umons.ac.be
  subcommand: sbatch
  progs:
    orca:
      set_env:
        - module --force purge
        - module load orca/4.0.1.2 
      command: /usr/local/orca/orca_4_0_1_2_linux_x86-64_openmpi202/orca
      partition:                # needed to define the different calculation requirements depending on the size of the molecule, use the "sinfo" command on the cluster to see all partitions
        small:
          name: Def
          time: 5-00:00:00
          cores: 8
        medium:
          bigindex_limit: 1000
          name: Def
          time: 5-00:00:00
          cores: 16
        big:
          bigindex_limit: 1500
          name: Long
          time: 15-00:00:00
          cores: 32  

dragon2:
  name: dragon2
  host: UMons
  address: dragon2.umons.ac.be  
  subcommand: sbatch
  progs:
    orca:
      set_env:
        - module load ORCA/4.0.1-OpenMPI-2.0.2 
      command: /opt/cecisw/arch/easybuild/2018b/software/ORCA/4.0.1-OpenMPI-2.0.2/orca
      partition:
        small:
          name: batch
          time: 5-00:00:00
          cores: 8
        medium:
          bigindex_limit: 1000
          name: batch
          time: 5-00:00:00
          cores: 16
        big:
          bigindex_limit: 1500
          name: long
          time: 15-00:00:00
          cores: 32

lemaitre3:
  name: lemaitre3
  host: UCL
  address: lemaitre3.cism.ucl.ac.be
  subcommand: sbatch
  progs:
    orca:
      set_env: 
        - module load ORCA/4.1.0-OpenMPI-3.1.3
      command: /opt/cecisw/arch/easybuild/2018b/software/ORCA/4.1.0-OpenMPI-3.1.3/orca
      partition:
        small:
          name: batch
          time: 2-00:00:00
          cores: 16
        medium:
          bigindex_limit: 1000

vega:
  name: vega
  host: ULB
  address: vega.ulb.ac.be
  subcommand: sbatch
  progs:
    orca:
      set_env: 
        - module purge
        - module load ORCA/4.0.0.2-OpenMPI-2.0.2
      command: /apps/brussel/interlagos/software/ORCA/4.0.0.2-OpenMPI-2.0.2/orca
      partition:
        small:
          name: defq
          time: 5-00:00:00
          cores: 8
        medium:
          bigindex_limit: 1000
          name: defq
          time: 14-00:00:00
          cores: 16

#Careful, not member of the CECI and uses Torque instead of SLURM
hydra:
  name: hydra
  host: ULB
  address: hydra.ulb.ac.be
  mail-type: a
  subcommand: qsub
  progs:
    qchem:
      set_env: 
        - module load Q-Chem/5.2.1-intel-2019a-mpich3
      command: qchem -nt
      partition:
        small:
          time: 120:00:00
          cores: 8
        medium:
          bigindex_limit: 500
          time: 120:00:00
          cores: 16

# Exception: for tiny jobs, the cluster doesn't matter, we just need to specify how much time and how many cores we want.
dummy:
  partition:
    tiny:
      bigindex_limit: 50     
      time: 00:05:00
      cores: 2