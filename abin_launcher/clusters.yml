# CECI Clusters relevant informations, see http://www.ceci-hpc.be/clusters.html for more details 
# This is a YAML database, see https://yaml.org/ for more informations

#! Keep in mind the definition of your scaling function when assigning values to the scale_limit keys

# Those keys are shared between the clusters, they will be used for tiny (or debug) jobs for which the cluster doesn't matter
shared: 
  progs:
    orca: &tiny_orca
      label: tiny
      scale_limit: 125000
      partition_name: default
      time: 00:05:00
      cores: 2 
    qchem: &tiny_qchem
      label: tiny
      scale_limit: 125000
      partition_name: default
      time: 00:05:00
      cores: 2 

# Below are all the useful informations specific to each cluster and each program you wish to run on this cluster.

dragon1:
  name: dragon1
  host: UMons
  address: dragon1.umons.ac.be
  subcommand: sbatch
  progs:
    orca:
      set_env:
        - module --force purge
        - module load orca/4.0.1.2 
      command: /usr/local/orca/orca_4_0_1_2_linux_x86-64_openmpi202/orca
      job_scales: 
        - <<: *tiny_orca     
        - label: small
          scale_limit: 1000
          partition_name: Def
          time: 5-00:00:00
          cores: 8
        - label: medium
          scale_limit: 1500
          partition_name: Def
          time: 5-00:00:00
          cores: 16
        - label: big
          scale_limit: 100000
          partition_name: Long
          time: 15-00:00:00
          cores: 32  

dragon2:
  name: dragon2
  host: UMons
  address: dragon2.umons.ac.be  
  subcommand: sbatch
  progs:
    orca:
      set_env:
        - module load ORCA/4.0.1-OpenMPI-2.0.2 
      command: /opt/cecisw/arch/easybuild/2018b/software/ORCA/4.0.1-OpenMPI-2.0.2/orca
      job_scales:
        - <<: *tiny_orca
          partition_name: debug  
        - label: small
          scale_limit: 1000
          partition_name: batch
          time: 5-00:00:00
          cores: 8
        - label: medium
          scale_limit: 1500
          partition_name: batch
          time: 5-00:00:00
          cores: 16
        - label: big
          scale_limit: 100000
          partition_name: long
          time: 15-00:00:00
          cores: 32  

lemaitre3:
  name: lemaitre3
  host: UCL
  address: lemaitre3.cism.ucl.ac.be
  subcommand: sbatch
  progs:
    orca:
      set_env: 
        - module load ORCA/4.1.0-OpenMPI-3.1.3
      command: /opt/cecisw/arch/easybuild/2018b/software/ORCA/4.1.0-OpenMPI-3.1.3/orca
      job_scales:
        - <<: *tiny_orca  
        - label: small
          scale_limit: 1000
          partition_name: batch
          time: 2-00:00:00
          cores: 16

vega:
  name: vega
  host: ULB
  address: vega.ulb.ac.be
  subcommand: sbatch
  progs:
    orca:
      set_env: 
        - module purge
        - module load ORCA/4.0.0.2-OpenMPI-2.0.2
      command: /apps/brussel/interlagos/software/ORCA/4.0.0.2-OpenMPI-2.0.2/orca
      job_scales:
        - <<: *tiny_orca  
        - label: small
          scale_limit: 1e9
          partition_name: defq
          time: 5-00:00:00
          cores: 8
        - label: medium
          scale_limit: 3.3750E+09
          partition_name: defq
          time: 10-00:00:00
          cores: 16

#Careful, not member of the CECI and uses Torque instead of SLURM
hydra:
  name: hydra
  host: ULB
  address: hydra.ulb.ac.be
  mail-type: a
  subcommand: qsub
  progs:
    qchem:
      set_env: 
        - module load Q-Chem/5.2.1-intel-2019a-mpich3
      command: qchem -nt
      job_scales:
        - <<: *tiny_qchem  
        - label: small
          scale_limit: 500
          partition_name: dummy
          time: 120:00:00
          cores: 8
        - label: medium
          scale_limit: 1000
          partition_name: dummy
          time: 120:00:00
          cores: 16
  